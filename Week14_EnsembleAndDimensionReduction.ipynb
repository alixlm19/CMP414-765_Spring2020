{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 14\n",
    "# Ensemble Learning and Data Dimension Reduction\n",
    "\n",
    "*Readings: Chapter 7, 8*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning\n",
    "\n",
    "The goal of ensemble learning is to combine the predictions of several base estimators in order to improve generalizability and robustness\n",
    "\n",
    ">**Wisdom of the crowd:** the collective opinion of a group of individuals is often better than that of a single expert.\n",
    "\n",
    "### Popular Ensemble methods\n",
    "- Build several estimators *independently* and then average their predictions. On average, the combined estimator is usually better than any of the single base estimator.\n",
    "\n",
    "**Examples:** Bagging, random forests.\n",
    "- Build several estimators *sequentially* with each new one trying to reduce the bias of the previous estimators.\n",
    "\n",
    "**Examples:** AdaBoost, Gradient Boost, XGBoost\n",
    "- Build a model to aggregate the predictions of several other models. The second-level model learns to highlight where each base estimator performs best and discredit where it performs poorly.\n",
    "\n",
    "**Examples** Stacking, *neural networks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the moon dataset to illustrate the power of ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a moon-shape dataset\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=500, noise=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[:, 0], X[:500,1], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: A single decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a Decision Tree model to build a classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_clas = DecisionTreeClassifier(max_depth = 5)\n",
    "tree_clas.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy score on test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_test_pred = tree_clas.predict(X_test) # predict y value for test set\n",
    "score = accuracy_score(y_test, y_test_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision boundary\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    padding=0.1\n",
    "    res=0.01\n",
    "    \n",
    "    x_min,x_max=X[:,0].min(), X[:,0].max()\n",
    "    y_min, y_max=X[:,1].min(), X[:,1].max()\n",
    "    \n",
    "    x_range=x_max-x_min\n",
    "    y_range=y_max-y_min\n",
    "    \n",
    "    xx,yy= np.meshgrid(np.arange(x_min,x_max,res),np.arange(y_min,y_max,res))\n",
    "    \n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    cs = plt.contourf(xx,yy,Z, cmap=plt.cm.Accent)\n",
    "    \n",
    "    plt.scatter(X[:,0], X[:,1],s=35,c=y,cmap=plt.cm.cool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(tree_clas,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Combining decision tree, logistic regression, and linear SVM\n",
    "\n",
    "Let's train three classifiers and let them vote for the final decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import classifiers\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "lr_clf = LogisticRegression(solver='lbfgs')\n",
    "svm_clf = SVC(gamma='auto')\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', lr_clf),\n",
    "                ('svm', svm_clf),\n",
    "                ('dt', dt_clf)],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy score for each model\n",
    "for clf in (lr_clf, svm_clf, dt_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, \n",
    "          accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Bagging and Pasting\n",
    "Another approach is to use the same training algorithm for every classifier, but to train them on different random subsets of the training set.\n",
    "- When sampling is performed *with* replacement, this method is called **bagging**.\n",
    "- When sampling is performed *without* replacement, it is called **pasting**.\n",
    "\n",
    "Bagging and pasting allow training instances to be sampled several times across multiple classifiers, but only bagging allows training instances to be sampled several times for the same classifier.\n",
    "\n",
    "Once all base models are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all models. The aggregation is typically the **majority vote** for classification, or the **average** for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an ensemble of 500 decision trees\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    n_estimators=500,\n",
    "    max_samples=100,\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy score\n",
    "y_test_pred = bag_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision boundary\n",
    "X1 = np.linspace(-2, 3, 100)\n",
    "X2 = np.linspace(-1.5, 2, 100)\n",
    "X1_mesh, X2_mesh = np.meshgrid(X1, X2)\n",
    "X_new = np.c_[X1_mesh.ravel(), X2_mesh.ravel()]\n",
    "\n",
    "y_pred = bag_clf.predict(X_new).reshape(X1_mesh.shape)\n",
    "\n",
    "plt.contourf(X1,\n",
    "             X2,\n",
    "             y_pred,\n",
    "             alpha=0.3)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "Boosting refers to Ensemble methods that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "AdaBoost, short for *Adaptive Boosting*, is a boosting methods where each predictor is trained with more attention to the training instances that its predecessor underfitted.\n",
    "\n",
    "### AdaBoost algorithm\n",
    "- Train a base classifier (such as a Decision Tree)\n",
    "- Calculate the accuracy of the base classifier and find all misclassified training instances\n",
    "- Increase the weights of the misclassified training instances\n",
    "- Train a second base classifier\n",
    "- Repeat Step 2-4\n",
    "\n",
    "The **weight** of a training instance refers to the factor of its error term in the cost function.\n",
    "\n",
    "Once all predictors are trained, the ensemble makes predictions by taking votes from each base predictor. Unlike bagging/pasting methods, predictors have different weights depending on their overall accuracy on the weighted training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration, this method tries to fit the new predictor to the *residual errors* made by the previous predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for regression\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3 * X[:, 0] ** 2 + 0.05 * np.random.randn(100)\n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a base Decision Tree regressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dt1 = DecisionTreeRegressor(max_depth=2)\n",
    "dt1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the regression line\n",
    "\n",
    "def plot_predictions(regressors,\n",
    "                     X,\n",
    "                     y,\n",
    "                     axes,\n",
    "                     label=None,\n",
    "                     style='r-',\n",
    "                     data_label=None,\n",
    "                     data_style='b.'):\n",
    "    # Create a input array\n",
    "    x1 = np.linspace(axes[0], axes[1], 500)\n",
    "    \n",
    "    # Obtain model prediction\n",
    "    y_pred = sum(regressor.predict(x1.reshape(-1, 1))\\\n",
    "                 for regressor in regressors)\n",
    "    \n",
    "    # Plot data points\n",
    "    plt.plot(X[:, 0], y, data_style, label=data_label)\n",
    "    \n",
    "    # Plot regression line\n",
    "    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n",
    "    \n",
    "    plt.legend(loc='upper center', fontsize=16)\n",
    "    plt.axis(axes)\n",
    "\n",
    "plot_predictions([dt1], X, y,\n",
    "                 axes=[-0.5, 0.5, -0.1, 0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the residual errors made by this regressor\n",
    "y2 = y - dt1.predict(X)\n",
    "plt.scatter(X, y2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build another Decision Tree to fit y2\n",
    "dt2 = DecisionTreeRegressor(max_depth=2)\n",
    "dt2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the second regressor\n",
    "plot_predictions([dt2], X, y2,\n",
    "                 axes=[-0.5, 0.5, -0.3, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the ensemble\n",
    "plot_predictions([dt1, dt2], X, y,\n",
    "                 axes=[-0.5, 0.5, -0.1, 0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model = GradientBoostingRegressor(max_depth=2,\n",
    "                                  n_estimators=3,\n",
    "                                  learning_rate=1.0)\n",
    "model.fit(X, y)\n",
    "plot_predictions([model], X, y, axes=[-0.5, 0.5, -0.1, 0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- learning_rate shrinks the contribution of each tree, reducing model overfitting\n",
    "- subsample: limiting the fraction of samples to be used for fitting each individual tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "Stacking is based on a simple idea: instead of using trivial function (such as hard voting), why don't we train a model to aggregate the predictions of all base predictors? A common approach is to split the training set into two subsets. The first subset is used to train each base predictors, and the second subset is used to train the *meta learner*.\n",
    "\n",
    "### Stacking algorithm\n",
    "- Split the training set into $S_1$ and $S_2$.\n",
    "- Train each base predictor by fitting it to $S_1$.\n",
    "- Record the prediction of each base predictor on $S_2$.\n",
    "- Use these predictions as input, train the meta learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "Many Machine Learning problems involve thousands or even millions of features for each training instance. Not only does this make training extremely slow, it can also make it much harder to find a good solution. This problem is often referred to as the **curse of dimensionality**.\n",
    "\n",
    "Fortunately, it is often possible to reduce the number of features considerably, turning an intractable problem into a tractable one. This procedure is called **dimensionality reduction**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main approaches for dimensionality reduction\n",
    "- Projection: remove features which have little variation.\n",
    "- Manifold learning: Use a few parameters to describe the dataset.\n",
    "- Principal Component Analysis (PCA): use matrix decomposition techniques to find the most important feature combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training set and show its first 5 rows.\n",
    "import pandas as pd\n",
    "path = \"C:\\\\Users\\\\ch002\\\\Dropbox\\\\Teaching\\\\CMP414\\\\Data\\\\mnist-in-csv\\\\\"\n",
    "filename = \"mnist_train.csv\"\n",
    "raw_data = pd.read_csv(path + filename, delimiter = ',')\n",
    "X = raw_data.iloc[:, 1:].to_numpy()\n",
    "y = raw_data.iloc[:, 0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X[0, :].reshape(28, 28),\n",
    "           cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use PCA to reduce the number of features\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=150)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "print('X shape:', X.shape)\n",
    "print('X_reduced shape:', X_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover the data from compressed representation\n",
    "X0 = pca.inverse_transform(X_reduced[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the recovered image\n",
    "plt.imshow(X0.reshape(28, 28),\n",
    "           cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Summary\n",
    "\n",
    "## Python libraries\n",
    "- numpy\n",
    "- pandas\n",
    "- matplotlib\n",
    "- seaborn\n",
    "- sklearn\n",
    "- tensorflow\n",
    "\n",
    "## Models\n",
    "- Linear regression\n",
    "- Polynomial regression\n",
    "- Logistic regression\n",
    "- K-nearest neighbour\n",
    "- Support Vector Machine\n",
    "- Neural Network\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Clustering methods\n",
    "- Bagging\n",
    "- Boosting\n",
    "- Stacking\n",
    "- Principle component analysis\n",
    "\n",
    "## Data processing/visualization techniques\n",
    "- Data cleaning\n",
    "- Missing data imputation\n",
    "- Dimensionality reduction\n",
    "- Descriptive statistics (mean, median, variation, std)\n",
    "- Histogram\n",
    "- Scattor plot\n",
    "- Confusion matrix\n",
    "- ROC curve and AUC\n",
    "- Decision boundary\n",
    "- Regression line\n",
    "\n",
    "## Model validation/fine tuning\n",
    "- Cross validation\n",
    "- Regularization (L1, L2, Elastic Net)\n",
    "- Grid search\n",
    "- Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
