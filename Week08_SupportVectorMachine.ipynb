{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 \n",
    "# Support Vector Machines\n",
    "\n",
    "*Support Vector Machine* (SVM) is one of the most popular models in Machine Learning. It is capable of performing linear or nonlinear classification, regression, and even outlier detection. This lecture will explain the core concepts of SVMs, how to use them, and how they work.\n",
    "\n",
    "## Task 1: Linear SVM with Hard Margin\n",
    "- Each data example has two features: $x_1$ and $x_2$. Using them as coordinates, they can be visualized as a data point on the coordinate plane.\n",
    "- Binary classification: target value $y = 1$ means that the instance belongs to a certain class (class 1); $y = -1$ means that the instance belongs to another class (class -1).\n",
    "- Classes are linearly separable: The two classes can clearly be separrated with a straight line.\n",
    "- The goal is to find a straight line that best separates the two classes perfectly (no mis-classification is allowed).\n",
    "- The best straight line that separates the two classes is the one with maximized distance from it to the nearest data point on each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/ch00226855/CMP464-788-Spring2019/raw/master/Data/SVM1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above graph, there are three lines (H1, H2, H3) that try to separate black dots from white dots.\n",
    "- H1 is clearly bad because it doesn't even separate the two classes properly.\n",
    "- H2 separates the two classes perfectly, but it is so close to the data points that it will probably not perform well on new instances.\n",
    "- H3 not only separates the two classes but also stays as far away from the closest training instances as possible. It is reasonable to believe that H3 will generalize well on new instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/ch00226855/CMP464-788-Spring2019/raw/master/Data/SVM2.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that adding more training instances \"off the street\" will not affect the decision boundary at all: it is fully determined (or \"supported\") by the instances located on the edge of the street. These instances are called the **support vectors**.\n",
    "\n",
    "SVMs are sensitive to the feature scales, so proper feature scaling is necessary for obtaining a good decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Express an SVM model with three parallel lines\n",
    "\n",
    "We can use three lines to expression the above binary classifier:\n",
    "\n",
    "- $w_1x_1 + w_2x_2 + b = 0$ represents the line in the middle of the gap.\n",
    "- $w_1x_1 + w_2x_2 + b = 1$ represents the right boundary of the gap.\n",
    "- $w_1x_1 + w_2x_2 + b = -1$ represents the left boundary of the gap.\n",
    "\n",
    "The parameters of this model are $w_1(\\ge 0), w_2, b$. To avoid ambiguity, we choose $w_1$ to be non-negative. Here all three lines share the same slope w because they are parallel. \n",
    "\n",
    "- For any point $(x_1, x_2)$ on the right half plane divided by the central line, the expression $w_1x_1 + w_2x_2 + b$ is positive.\n",
    "- For any point $(x_1, x_2)$ on the left half plane, $w_1x_1 + w_2x_2 + b$ is negative.\n",
    "\n",
    "**Question**: How to compute the slope of these lines?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose w1 = 1, w2 = 2, and b = -3, draw the above\n",
    "# three lines on a graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM: Decision function and predictions\n",
    "\n",
    "The linear SVM classifier model predicts the class of a new instance $(x_1, x_2)$ by simply computing the decision function $w_1x_1 + w_2x_2 + b$: if the result is positive, the predicted class is the class on the right, otherwise it is the class on the left.\n",
    "\n",
    "**Decision rule**\n",
    "\\begin{equation}\n",
    "\\hat{y}(prediction)  = \n",
    "\\left\\{\n",
    "\\begin{array}{cc}\n",
    " 1 & \\textit{if } w_1x_1 + w_2x_2 + b \\ge 0,\\\\\n",
    " -1 & \\textit{if } w_1x_1 + w_2x_2 + b < 0.\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose w1 = 1, w2 = 2, and b = -3, decide the \n",
    "# class of: \n",
    "# 1) x_1 = 0, x_2 = 1; \n",
    "# 2) x_1 = 2, x_2 = 3;\n",
    "# 3) x_1 = 1, x_2 = 1.1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the points together with three decision lines.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM: Training objective\n",
    "Let's explore how does the magnitude of parameters affect the margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the previous model, increase w1 from 1 to 2,\n",
    "# and plot the new decision margin.\n",
    "# lines are: 2 * x1 + 2 * x2 - 3 = (0, 1, -1)\n",
    "\n",
    "plt.title(r\"$2x_1 + 2x_2 - 3 = 0$\")\n",
    "plt.xlim([-3, 3])\n",
    "plt.ylim([0, 5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decrease w2 from 2 to 1 from the original model,\n",
    "# and plot the new decision margin.\n",
    "\n",
    "plt.title(r\"$x_1 + x_2 - 3 = 0$\")\n",
    "plt.xlim([-3, 3])\n",
    "plt.ylim([0, 5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change b from -3 to -2 from the original model,\n",
    "# and plot the new decision margin\n",
    "\n",
    "plt.title(r\"$x_1 + 2x_2 - 2 = 0$\")\n",
    "plt.xlim([-3, 3])\n",
    "plt.ylim([0, 5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does each parameter affect the width of decision margin?**\n",
    "- smaller w_1 -> wider gap; larger w_1 -> narrower gap.\n",
    "- smaller w_2 -> wider gap; larger w_2 -> narrower gap.\n",
    "- b does not affect the width of the gap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawbacks of Hard Margin SVM\n",
    "If we strictly impose that all instances be off the street and on the correct side, the model can only be applied to models that are linearly separable. Moreover, it will be very sensitive to outliers. The above figure illustrates how badly one outlier may affect the model. \n",
    "\n",
    "<img src=\"https://github.com/ch00226855/CMP464-788-Spring2019/raw/master/Data/SVM3.png\">\n",
    "\n",
    "To avoid these issues, it is preferable to use a more flexible model. The objective is to find a good balance between keeping the street as wide as possible and limiting the margin violations. This is called *soft margin classification*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Linear SVM with Soft Margin\n",
    "- Each data example has two features: $x_1$ and $x_2$.\n",
    "- Binary classification: target value $y = 1$ means that the instance belongs to \"class 1\", $y = -1$ means that the instance belongs to \"class -1\".\n",
    "- Classes are **mostly** linearly separable: **with a few exceptions**, the two classes can be separrated with a straight line.\n",
    "- The goal is to find a straight line that best separates the two classes. **Mis-classifications are allowed, but each mis-classification will add a cost to the model.**\n",
    "- The objective function takes into account both **the magnitude of w's (how wide the gap is) and the degree of margin violations**.\n",
    "\n",
    "<img src=\"https://github.com/ch00226855/CMP464-788-Spring2019/raw/master/Data/SVM4.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of Soft Margin SVM\n",
    "- The dataset does not have to be linearly separable.\n",
    "- Outliers does not affect the model too much.\n",
    "- The cost function is convex and it has no constraints, thus gradient descent can be applied to its minimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Linear SVM\n",
    "- Feature scaling: are all features distributed similarly?\n",
    "- $C$: how much does penalty matter?\n",
    "- Class weights: are all classes equally important?\n",
    "- Multiple classes: One vs. One, or One vs. Rest?\n",
    "- **Kernel SVM**: Allow non-linear decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel (Non-Linear) SVM: Motivation\n",
    "- Data may not be linearly separable in their original features.\n",
    "\n",
    "Consider the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100  # size of data set\n",
    "data = pd.DataFrame(index=np.arange(m))\n",
    "# input features: x1, x2\n",
    "data['x1'] = np.random.randn(m)\n",
    "data['x2'] = np.random.randn(m)\n",
    "# target value: y\n",
    "# y = 0 if (x1, x2) is inside the unit circle\n",
    "# y = 1 if (x1, x2) is outside of the unit circle\n",
    "data['y'] = (data['x1'] ** 2 + data['x2'] ** 2 < 1)\n",
    "\n",
    "# plot the data\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(data['x1'],\n",
    "            data['x2'],\n",
    "            c=data['y'])\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(-2, 2)\n",
    "plt.grid(True, which='both')\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.axvline(x=0, color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with Polynomial Kernel\n",
    "This dataset is clearly not linear separable. Next, add two new features $x_1^2$ and $x_2^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add two new features: x1 squared and x2 squared\n",
    "data['x1^2'] = data['x1'] ** 2\n",
    "data['x2^2'] = data['x2'] ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(data['x1^2'],\n",
    "            data['x2^2'],\n",
    "            c=data['y'])\n",
    "plt.grid(True, which='both')\n",
    "plt.xlabel('x1 squared')\n",
    "plt.ylabel('x2 squared')\n",
    "plt.xlim(0, 4)\n",
    "plt.ylim(0, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC(C=10,\n",
    "            kernel='poly',\n",
    "            degree=2,\n",
    "            coef0=0)\n",
    "# The hyperparameter coef0 controls how much the\n",
    "# model is influenced by high-degree polynomials\n",
    "# versus low-degree polynomials\n",
    "model.fit(data[['x1', 'x2']], data['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\">sklearn.svm.SVC</a>\n",
    "\n",
    "- Parameters\n",
    "- Attributes\n",
    "- Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the decision boundary\n",
    "x1s = np.linspace(-2, 2, 100)\n",
    "x2s = np.linspace(-2, 2, 100)\n",
    "x1, x2 = np.meshgrid(x1s, x2s)\n",
    "X = np.c_[x1.reshape(-1, 1), x2.reshape(-1, 1)]\n",
    "y_pred = model.predict(X).reshape(x1.shape)\n",
    "plt.contourf(x1, x2, y_pred,\n",
    "             cmap=plt.cm.brg,\n",
    "             alpha=0.2)\n",
    "y_decision = model.decision_function(X).reshape(x1.shape)\n",
    "plt.contourf(x1, x2, y_decision,\n",
    "             cmap=plt.cm.brg,\n",
    "             alpha=0.1)\n",
    "plt.scatter(data['x1'], data['x2'], c=data['y'])\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.axvline(x=0, color='k')\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(-2, 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of kernels\n",
    "Consider a dataset with $m$ examples, each having $n$ features.\n",
    "1. If $m < n$, then it is advised to use no kernel (sometimes called *linear kernel*) to avoid overfitting.\n",
    "2. If $m$ is slightly larger than $n$, use a kernel (Gaussian RBF kernel is generally better than polynomial kernel in practice)\n",
    "3. If $m$ is extremely large, it is usually a good idea to create new features.\n",
    "\n",
    "Other choices of kernels: \n",
    "- sigmoid kernel (equivalent to logistic regression)\n",
    "- string kernel (for text data)\n",
    "- define new kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Kernel Trick\n",
    "\n",
    "Adding extra features may dramatically increase the computational complexity. For example, consider a polynomial kernel with degree 3 and set coef0 = 1. This means to transform each input feature x as:\n",
    "\n",
    "$(x+1)^3 = x^3 + 3x^2 + 3x + 1$,\n",
    "\n",
    "and two new features 'x^3', 'x^2' are added to the model. If the dataset is very complex and a large degree is needed, the model will create a huge number of features, making the model too slow.\n",
    "\n",
    "Fortunately, when using SVMs you can apply an almost miraculous mathematical technique called the *kernel trick*, which makes it possible to handle extra features without actually having to add them. As a result, the exponential explosion of complexity is avoided since new features are not actually added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
