{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 12\n",
    "# Image Classification with Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas\n",
    "- Dense layers may contain redudent connections\n",
    "- Some information should be invariant to spacial translation\n",
    "- The number of parameters can be reduced if certain weights share the same value.\n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcS4LZdFg5QPbgDb-jvP-YT0N51eRkWg45uF0ybsB5k0Ubr0-gOC&usqp=CAU\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Convolution Layer\n",
    "<img src=\"https://cdn-media-1.freecodecamp.org/images/Gjxh-aApWTzIRI1UNmGnNLrk8OKsQaf2tlDu\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2D smoothing with Gaussian kernel**\n",
    "\n",
    "<img src=\"https://www.cs.umd.edu/class/fall2016/cmsc426/matlab/filters/html/filters_tutorial_03.png\" width=\"400\"><img src=\"https://www.mathworks.com/help/examples/stats/win64/ComputeTheMultivariateNormalPdfExample_01.png\" width=\"200\">\n",
    "\n",
    "**Edge detection**\n",
    "\n",
    "<img src=\"https://aishack.in/static/img/tut/conv-line-detection-horizontal-result.jpg\" width=\"400\"><img src=\"https://www.researchgate.net/profile/Ching-Wei_Wang/publication/221472523/figure/fig5/AS:305540338077700@1449857901164/Convolution-filter-for-simple-edge-detect.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet5 on MNIST\n",
    "\n",
    "Yann LeCun, Leon Bottou, Yosuha Bengio and Patrick Haffner proposed a neural network architecture for handwritten and machine-printed character recognition in 1990â€™s which they called LeNet-5. It is one of the early example of a convolutional neural network\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/4308/1*1TI1aGBZ4dybR6__DI9dzA.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max-Pooling Layer\n",
    "<img src=\"https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "# import tensorflow.keras as K\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare the MNIST dataset.\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Convert the data from integers to floating-point numbers\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = tf.keras.Sequential()\n",
    "\n",
    "model_cnn.add(tf.keras.layers.Conv2D(filters=6,\n",
    "                                 kernel_size=(3, 3),\n",
    "                                 activation='relu',\n",
    "                                 input_shape=(28, 28, 1)))\n",
    "\n",
    "model_cnn.add(tf.keras.layers.Conv2D(filters=16,\n",
    "                                 kernel_size=(3, 3),\n",
    "                                 activation='relu'))\n",
    "\n",
    "model_cnn.add(tf.keras.layers.AveragePooling2D())\n",
    "\n",
    "model_cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model_cnn.add(tf.keras.layers.Dense(units=120,\n",
    "                       activation='relu'))\n",
    "model_cnn.add(tf.keras.layers.Dense(units=84,\n",
    "                       activation='relu'))\n",
    "model_cnn.add(tf.keras.layers.Dense(units=10,\n",
    "                       activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 6)         60        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 16)        880       \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               276600    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 288,554\n",
      "Trainable params: 288,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 39s 654us/sample - loss: 1.5999 - accuracy: 0.8634\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 40s 672us/sample - loss: 1.4915 - accuracy: 0.9706\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 39s 650us/sample - loss: 1.4845 - accuracy: 0.9769- loss: 1.4845 - accuracy\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 32s 535us/sample - loss: 1.4803 - accuracy: 0.9812\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 31s 516us/sample - loss: 1.4773 - accuracy: 0.9841\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 31s 514us/sample - loss: 1.4751 - accuracy: 0.9862\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 32s 527us/sample - loss: 1.4742 - accuracy: 0.9872\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 31s 522us/sample - loss: 1.4734 - accuracy: 0.9878\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 32s 535us/sample - loss: 1.4721 - accuracy: 0.9891\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 31s 519us/sample - loss: 1.4722 - accuracy: 0.9890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f964687cc8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn.fit(x_train.reshape(list(x_train.shape) + [1]), y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 184us/sample - loss: 1.4746 - accuracy: 0.9866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4746006938934326, 0.9866]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn.evaluate(x_test.reshape(list(x_test.shape) + [1]), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout and Model Regularization\n",
    "For a complicated model like deep neural networks, a major concern on its performance is model overfitting:\n",
    "![underfitting and overfitting](https://cdn-images-1.medium.com/max/1200/1*cdvfzvpkJkUudDEryFtCnA.png)\n",
    "In plain words, overfitting happens when the model is **memorizing** the training data, and become poorly at **generalizing** what they've learned to unseen data. Think about a student who memorized the entire machine learning textbook. He may appear quite knowledgable in machine learning when asked things directly from the book, but there is no way he can perform a machine project on a dataset not mentioned in the book.\n",
    "\n",
    "### How to dentify model overfitting?\n",
    "- Visualize the model (decision boundary, regression curves, etc.)\n",
    "- Observe the trends in training loss and the testing loss\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*vuZxFMi5fODz2OEcpG-S1g.png)\n",
    "\n",
    "### How to prevent model overfitting?\n",
    "1. Start with a simple model\n",
    "![](https://image.slidesharecdn.com/lawsofwebdesign-091104020153-phpapp01/95/laws-of-web-development-11-728.jpg?cb=1257384621)\n",
    "2. Add penalty to complicated models\n",
    "    - L1 Regularizor\n",
    "    - L2 Regularizor\n",
    "    - Elastic Net\n",
    "\n",
    "3. (For Neural Networks) Dropout layers: remove weights to the next layer\n",
    "![](https://cdn-images-1.medium.com/max/1800/1*iWQzxhVlvadk6VAJjsgXgg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with CIFAR-10 Dataset\n",
    "[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) is a widely used benchmark dataset for image classifiers. The dataset consists of 10 classes of color images of size $32\\times 32$. Let's build a neural network with **convolutional layers** to classify the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset\n",
    "- Use `request` to download the tar file from [https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)\n",
    "- Use `tarfile` to extract files\n",
    "- Use `pickle` to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading CIFAR10 dataset...\n",
      "Writing to file cifar-10-python.tar.gz ...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "filename = \"cifar-10-python.tar.gz\"\n",
    "if not os.path.isfile(filename):\n",
    "    print(\"Downloading CIFAR10 dataset...\")\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    file = requests.get(url)\n",
    "\n",
    "    print(\"Writing to file\", filename, \"...\")\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(file.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['batches.meta',\n",
       " 'data_batch_1',\n",
       " 'data_batch_2',\n",
       " 'data_batch_3',\n",
       " 'data_batch_4',\n",
       " 'data_batch_5',\n",
       " 'readme.html',\n",
       " 'test_batch']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tarfile\n",
    "datapath = \"cifar-10-batches-py/\" \n",
    "if not os.path.isdir(datapath):\n",
    "    print(\"Extracting files...\")\n",
    "    tar = tarfile.open(filename)\n",
    "    tar.extractall()\n",
    "    print(\"Files extracted.\")\n",
    "    tar.close()\n",
    "os.listdir(datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is broken into batches to prevent your machine from running **out of memory**. The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature size: (10000, 32, 32, 3)\n",
      "label size: 10000\n"
     ]
    }
   ],
   "source": [
    "# load one batch\n",
    "import pickle\n",
    "with open(datapath + \"data_batch_1\", \"rb\") as f:\n",
    "    batch = pickle.load(f, encoding=\"latin1\")\n",
    "    features = batch['data'].reshape([len(batch['data']), 3, 32, 32]).transpose(0, 2, 3, 1)\n",
    "    labels = batch['labels']\n",
    "print(\"feature size:\", features.shape)\n",
    "print(\"label size:\", len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label data is just a list of 10000 numbers in the range 0-9, which corresponds to each of the 10 classes in CIFAR-10. \n",
    "\n",
    "* **airplane**\n",
    "* **automobile**\n",
    "* **bird**\n",
    "* **cat**\n",
    "* **deer**\n",
    "* **dog**\n",
    "* **frog**\n",
    "* **horse**\n",
    "* **ship**\n",
    "* **truck**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'frog')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show a sample image\n",
    "sample_id = 243\n",
    "plt.imshow(features[sample_id])\n",
    "label_names = ['airplane', 'automobile', 'bird',\n",
    "            'cat', 'deer', 'dog', 'frog',\n",
    "            'horse', 'ship', 'truck']\n",
    "plt.xlabel(label_names[labels[sample_id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (10000, 32, 32, 3)\n",
      "labels shape: 10000\n",
      "features shape: (10000, 32, 32, 3)\n",
      "labels shape: 10000\n",
      "features shape: (10000, 32, 32, 3)\n",
      "labels shape: 10000\n",
      "features shape: (10000, 32, 32, 3)\n",
      "labels shape: 10000\n",
      "features shape: (10000, 32, 32, 3)\n",
      "labels shape: 10000\n",
      "train_features shape: (50000, 32, 32, 3)\n",
      "train_labels shape: (50000,)\n"
     ]
    }
   ],
   "source": [
    "# Load all images from batch 1-5\n",
    "train_features = np.empty([0, 32, 32, 3], dtype=np.uint8)\n",
    "train_labels = np.empty([0])\n",
    "for k in range(1, 6):\n",
    "    with open(datapath + \"data_batch_\" + str(k), \"rb\") as f:\n",
    "        batch = pickle.load(f, encoding=\"latin1\")\n",
    "        features = batch[\"data\"].reshape([len(batch['data']), 3, 32, 32]).transpose(0, 2, 3, 1)\n",
    "        labels=batch['labels']\n",
    "        print(\"features shape:\", features.shape)\n",
    "        print(\"labels shape:\", len(labels))\n",
    "        train_features = np.append(train_features, features, axis=0)\n",
    "        train_labels = np.append(train_labels, labels, axis=0)\n",
    "print(\"train_features shape:\", train_features.shape)\n",
    "print(\"train_labels shape:\", train_labels.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build CNN model\n",
    "### Create Convolutional Model\n",
    "\n",
    "The entire model consists of 14 layers in total. In addition to layers below lists what techniques are applied to build the model.\n",
    "\n",
    "1. Convolution with 64 different filters in size of (3x3)\n",
    "2. Max Pooling by 2\n",
    "  - ReLU activation function \n",
    "3. Convolution with 128 different filters in size of (3x3)\n",
    "4. Max Pooling by 2\n",
    "  - ReLU activation function \n",
    "5. Convolution with 256 different filters in size of (3x3)\n",
    "6. Max Pooling by 2\n",
    "  - ReLU activation function \n",
    "7. Convolution with 512 different filters in size of (3x3)\n",
    "8. Max Pooling by 2\n",
    "  - ReLU activation function \n",
    "9. Flattening the 3-D output of the last convolutional operations.\n",
    "10. Fully Connected Layer with 128 units\n",
    "  - Dropout \n",
    "11. Fully Connected Layer with 256 units\n",
    "  - Dropout \n",
    "12. Fully Connected Layer with 512 units\n",
    "  - Dropout \n",
    "13. Fully Connected Layer with 1024 units\n",
    "  - Dropout \n",
    "14. Fully Connected Layer with 10 units (number of image classes)\n",
    "\n",
    "the image below decribes how the conceptual convolving operation differs from the tensorflow implementation when you use [Channel x Width x Height] tensor format. \n",
    "\n",
    "<img src=\"https://adeshpande3.github.io/assets/Cover.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "num_predictions = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CNN model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), padding=\"same\",\n",
    "                              input_shape=features[0].shape,\n",
    "                              activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3),\n",
    "                              activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\",\n",
    "                              input_shape=features[0].shape,\n",
    "                              activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3),\n",
    "                              activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(512, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",\n",
    "#               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "        argument\n",
    "            - x: input image data in numpy array [32, 32, 3]\n",
    "        return\n",
    "            - normalized x \n",
    "    \"\"\"\n",
    "    min_val = np.min(x)\n",
    "    max_val = np.max(x)\n",
    "    x = (x-min_val) / (max_val-min_val)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_scaled = normalize(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "Epoch 1/10\n",
      " 2944/50000 [>.............................] - ETA: 2:46 - loss: 2.2703 - accuracy: 0.1611"
     ]
    }
   ],
   "source": [
    "model.fit(train_features_scaled, train_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
