{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression: Training Algorithm\n",
    "\n",
    "**Cost (loss) function** for logistic regression:\n",
    "\n",
    "\\begin{equation}\n",
    "c(\\theta) = \\left\\{\n",
    "\\begin{array}{cc}\n",
    "-\\log(\\hat{p}) & \\textit{if }y=1,\\\\\n",
    "-\\log(1-\\hat{p}) & \\textit{if }y=0.\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{equation}\n",
    "\n",
    "The cost function $c(\\theta)$:\n",
    "\n",
    "- small if $y=1$ (data example belongs to the class) and $\\hat{p}$ is close to 1.\n",
    "- small if $y=0$ (data example does not belong to the class) and $\\hat{p}$ is close to 0.\n",
    "- is a convex function, so that the gradient descent method always finds the minimum.\n",
    "\n",
    "**Uniformed expression for the cost function**:\n",
    "\n",
    "$J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^{m}\\big[y^{(i)}\\log(\\hat{p}^{(i)}) + (1-y^{(i)})\\log(1-\\hat{p}^{(i)})\\big]$\n",
    "\n",
    "- $c(\\theta) = J(\\theta)$ for $y=0$ and $y=1$.\n",
    "- There is no equivalent of the Normal Equation.\n",
    "- $J(\\theta)$ is a convex function.\n",
    "- $\\frac{\\partial J}{\\partial \\theta_j}=\\frac{1}{m}\\sum_{i=1}^{m}\\big(\\sigma(\\textbf{x}^{(i)}\\cdot\\theta^T) - y^{(i)}\\big)x_j^{(i)}$.\n",
    "\n",
    "**Question**: Why not use the mean-square-error (MSE) cost function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression: Varying The Threshold\n",
    "We can change the default threshold to improve classification accuracy of one particular class. The tradeoff usually is the reduced accuracy on the other class. \n",
    "- An **Receiver Operating Characteristics (ROC)** can be used to show such tradeoffs.\n",
    "    - x-axis: true positive rate (= true positive / (true positive + false negative))\n",
    "    - y-axis: false positive rate (= false positive / (true negative + false positive))\n",
    "- The **Area Under Curve (AUC)** score of the ROC curve is often used to measure the quality of the model:\n",
    "    - AUC close to 1: The model give satisfactory classification results for most choices of thresholds.\n",
    "    - AUC close to 0.5: The model does poorly for most thresholds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression for Multiple Classes (Softmax regression)\n",
    "**model**:\n",
    "\n",
    "$\\hat{p}_k = \\frac{\\exp(s_k(\\textbf{x}))}{\\sum_{i=1}^K\\exp(s_i(\\textbf{x}))}$.\n",
    "\n",
    "$s_k(\\textbf{x}) = \\textbf{x}\\cdot\\theta_k^T$\n",
    "\n",
    "- $\\hat{p}_k$ is the probability that the instance belongs to class $k$.\n",
    "- K is the number of classes.\n",
    "- $\\theta_k$ is the coefficient vector associated with class $k$. All these vectors are stored as rows in a parameter matrix $\\Theta$.\n",
    "- The softmax classifier predicts the class with the highest estimated probability (which is simply the class with the highest score).\n",
    "\n",
    "**Cross entropy cost function**\n",
    "\n",
    "$J(\\Theta) = -\\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^K\n",
    "y_k^{(i)}\\log(\\hat{p}_k^{(i)})$\n",
    "\n",
    "- $y_k^{(i)}$ is equal to 1 if the target for the i-th instance is $k$; otherwise, it is equal to 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Iris Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the training accuracy and testing accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 3-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve for each class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework:\n",
    "\n",
    "1. Divide the dataset randomly into 80% training set and 20% test set, and build a logistic classifier to identify Iris-Setosa using the petal width and petal length. \n",
    "2. Calculate test accuracy, precision, recall, f1-score.\n",
    "3. Plot the ROC curve and calculate AUC.\n",
    "4. (optional for undergraduates) Build a grid of points using `np.meshgrid` and use their probabilities to draw the decision boundary of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
