{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6\n",
    "# Multilinear Regression\n",
    "Last time we looked at a simple linear regression model $sales = \\beta_0 + \\beta_1\\cdot\\textit{TV advertising budget}$. More generally, a linear model makes a prediction by computing a weighted sum of their input features (plus a constant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilinear Regression: Model Assumptions\n",
    "**Model**:\n",
    "\n",
    "$\\hat{y} = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 +\\cdots + \\theta_nx_n$\n",
    "1. $\\hat{y}$ is the predicted value.\n",
    "2. $n$ is the number of features.\n",
    "3. $x_i$ is the i-th feature value.\n",
    "4. $\\theta_j$ is the j-th model parameter (associated with $x_j$).\n",
    "\n",
    "**Vectorized form**:\n",
    "\n",
    "**$\\hat{y} = \\textbf{x}\\cdot\\theta^T$**.\n",
    "1. $\\theta = (\\theta_0, \\theta_1, ..., \\theta_n)$ is the paramter vector.\n",
    "2. $\\textbf{x} = (1, x_1, ..., x_n)$ is the feature vector.\n",
    "\n",
    "**MSE (mean squared error) Cost function**:\n",
    "\n",
    "$J = \\frac{1}{m}\\sum_{i=1}^{m}\\big(\\textbf{x}^{(i)}\\cdot\\theta^T - y^{(i)}\\big)^2$\n",
    "\n",
    "Here $(\\textbf{x}^{(i)}, y^{(i)})$ represents the i-th training example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy example\n",
    "columns = ['Homework', 'Midterm', 'Final']\n",
    "data = pd.DataFrame({\n",
    "    \"Homework\": [95, 70, 80, 100, 70],\n",
    "    \"Midterm\": [90, 60, 80, 80, 85],\n",
    "    \"Final\": [93, 66, 85, 60, 90]\n",
    "}, index=[\"Alice\", \"Bob\", \"Clare\", \"David\", \"Eve\"])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: Suppose that another student Fred has Homework score 85 and Midterm score 80. What is prediction of his final exam score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial and Error\n",
    "Before studying the algorithms that give the best fit, let's see how well we can learn the model ourselves. Let's take the following approach:\n",
    "\n",
    "1. Calculate the correlation coefficients of variable pair (Homework, Final) and (Midterm, Final)\n",
    "2. Distribute weight to homework and final proportional to their correlation coefficients. For example, if the value are the same for both pairs, then both the term homework and midterm should have parameter value 0.5.\n",
    "3. Calculate the value of $\\theta_0$ so that Alice's data fits the model exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation coefficient between homework and final using np.corrcoef()\n",
    "\n",
    "\n",
    "# Calculate the correlation coefficient between midterm and final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribute weight to homework and midterm according to their correlation coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve for theta0 so that if homework=95 and midterm=90, final=93.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate the average error the model makes on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the MSE (mean-squared-error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilinear Regression: Normal Equation\n",
    "The value of $\\theta$ that minimizes the cost function is:\n",
    "\n",
    "$\\hat{\\theta} = \\big(\\textbf{X}^T\\cdot\\textbf{X}\\big)^{-1}\\cdot\\textbf{X}^T\\cdot\\textbf{y}$.\n",
    "\n",
    "1. $\\textbf{X}$ is an $m\\times n$ matrix whose i-th row is $\\textbf{x}^{(i)}$.\n",
    "$$\\textbf{X} = \\begin{pmatrix}\n",
    "1 & x^{(1)}_1 & x^{(1)}_2 & \\cdots & x^{(1)}_n \\\\\n",
    "1 & x^{(2)}_1 & x^{(2)}_2 & \\cdots & x^{(2)}_n \\\\\n",
    "\\vdots & \\vdots &\\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x^{(m)}_1 & x^{(m)}_2 & \\cdots & x^{(m)}_n \\\\\n",
    "\\end{pmatrix}$$\n",
    "2. $$\\textbf{y} = \\begin{pmatrix}y^{(0)} \\\\ \\vdots \\\\ y^{(m)}\\end{pmatrix}$$.\n",
    "3. The cost function $J(\\theta)$ also has a matrix expression\n",
    "$$J(\\theta) = \\frac{1}{m}(\\textbf{X}^T\\cdot\\theta - \\textbf{y})^T\\cdot (\\textbf{X}^T\\cdot\\theta - \\textbf{y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct matrix X using np.hstack(), np.ones()\n",
    "m, n = data.shape\n",
    "X = np.hstack([np.ones([m, 1]), data[['Homework', 'Midterm']].values])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct vector y\n",
    "y = data[['Final']].values\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the normal equation to find theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilinear Regression: Gradient Descent\n",
    "The normal equation is not applicable when $\\textbf{X}^T\\cdot\\textbf{X}$ is not invertible. It happens if:\n",
    "- Several features are linearly dependent (for example, feature3 = feature1 + feature2)\n",
    "- The number of features is greater than the number of training data (for example, DNA data)\n",
    "\n",
    "In this case, we can use the **gradient descent** method to minimize the cost function.\n",
    "\n",
    "Gradient descent is an iterative algorithm for finding the **local minimum** of a differentiable function.\n",
    "- Choose an initial value of $\\hat{\\theta}$ and a **learning rate** $r$.\n",
    "- For each iteration $k$, do:\n",
    "$$\\hat{\\theta} \\leftarrow \\hat{\\theta} - r\\cdot\\frac{\\partial J(\\hat{\\theta})}{\\partial \\theta}.$$\n",
    "- The partial derivative of the cost function is given by\n",
    "$$\n",
    "\\frac{\\partial J(\\hat{\\theta})}{\\partial \\theta} = \\frac{2}{m}\\cdot\\textbf{X}^T\\cdot(\\textbf{X}\\cdot\\theta - \\textbf{y}).\n",
    "$$\n",
    "- **Verify the formula of partial derivative asuuming there is one input feature.**\n",
    "\n",
    "- End iteration if certain stop criteria is reached, such as:\n",
    "    - Value of $\\hat{\\theta}$ becomes stable.\n",
    "    - Certain iteration amount is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a random initial value\n",
    "m, n = data.shape\n",
    "theta_hat = np.random.rand(n, 1)\n",
    "print(theta_hat)\n",
    "\n",
    "# normalize the input values so that numbers are between 0 and 1.\n",
    "X2 = np.hstack([np.ones([m, 1]), data[['Homework', 'Midterm']].values/100])\n",
    "y2 = y\n",
    "\n",
    "num_iter = 2000\n",
    "r = 0.01\n",
    "\n",
    "MSEs = []\n",
    "for iter in range(num_iter):\n",
    "    # Calculate the gradient\n",
    "    gradient = \n",
    "\n",
    "    # Update the parameters\n",
    "    theta_hat -= \n",
    "    \n",
    "    # Calculate the MSE\n",
    "    MSE = \n",
    "    \n",
    "    # Append the MSE to the list MSEs\n",
    "    MSEs.append(MSE)\n",
    "    \n",
    "print(theta_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the final MSE using its matrix expression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the curve of the cost function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "1. Change $r$ to 0.000001 and 1. Observe the MSE curve.\n",
    "2. Does the initial point matter?\n",
    "3. How to determine when to stop the iteration?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilinear Regression via `sklearn.linear_model.LinearRegression`\n",
    "- fit(): train the model\n",
    "- predict(): make predictions\n",
    "- coef_: variable coefficients\n",
    "- intercept_: model intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a linear regression model using LinearRegression from sklearn.linear_model\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the parameter values\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
