{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6\n",
    "# Multilinear Regression\n",
    "\n",
    "**Reference: Chapter 4**\n",
    "\n",
    "Last time we looked at a simple linear regression model $sales = \\beta_0 + \\beta_1\\cdot\\textit{TV advertising budget}$. More generally, a linear model makes a prediction by computing a weighted sum of their input features (plus a constant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilinear Regression: Model Assumptions\n",
    "**Model**:\n",
    "\n",
    "$\\hat{y} = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 +\\cdots + \\theta_nx_n$\n",
    "1. $\\hat{y}$ is the predicted value.\n",
    "2. $n$ is the number of features.\n",
    "3. $x_i$ is the i-th feature value.\n",
    "4. $\\theta_j$ is the j-th model parameter (associated with $x_j$).\n",
    "\n",
    "**Vectorized form**:\n",
    "\n",
    "**$\\hat{y} = \\textbf{x}\\cdot\\theta^T$**.\n",
    "1. $\\theta = (\\theta_0, \\theta_1, ..., \\theta_n)$ is the paramter vector.\n",
    "2. $\\textbf{x} = (1, x_1, ..., x_n)$ is the feature vector.\n",
    "\n",
    "**MSE (mean squared error) Cost function**:\n",
    "\n",
    "$J = \\frac{1}{m}\\sum_{i=1}^{m}\\big(\\textbf{x}^{(i)}\\cdot\\theta^T - y^{(i)}\\big)^2$\n",
    "\n",
    "Here $(\\textbf{x}^{(i)}, y^{(i)})$ represents the i-th training example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Homework</th>\n",
       "      <th>Midterm</th>\n",
       "      <th>Final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Alice</td>\n",
       "      <td>95</td>\n",
       "      <td>90</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Bob</td>\n",
       "      <td>70</td>\n",
       "      <td>60</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Clare</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>David</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Eve</td>\n",
       "      <td>70</td>\n",
       "      <td>85</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Homework  Midterm  Final\n",
       "Alice        95       90     93\n",
       "Bob          70       60     66\n",
       "Clare        80       80     85\n",
       "David       100       80     60\n",
       "Eve          70       85     90"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Toy example\n",
    "columns = ['Homework', 'Midterm', 'Final']\n",
    "data = pd.DataFrame({\n",
    "    \"Homework\": [95, 70, 80, 100, 70],\n",
    "    \"Midterm\": [90, 60, 80, 80, 85],\n",
    "    \"Final\": [93, 66, 85, 60, 90]\n",
    "}, index=[\"Alice\", \"Bob\", \"Clare\", \"David\", \"Eve\"])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case:\n",
    "- $x_1$ is the homework feature\n",
    "- $x_2$ is the midterm feature\n",
    "- $y$ is the final feature\n",
    "- model is: $final = \\theta_0 + \\theta_1 * homework + \\theta_2 * midterm$\n",
    "- We need to come up with values for $\\theta_0, \\theta_1, \\theta_2$ to complete the model.\n",
    "\n",
    "**Objective**: Suppose that another student Fred has Homework score 85 and Midterm score 80. What is prediction of his final exam score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial and Error\n",
    "Before studying the algorithms that give the best fit, let's see how well we can learn the model ourselves. Let's take the following approach:\n",
    "\n",
    "1. Calculate the correlation coefficients of variable pair (Homework, Final) and (Midterm, Final)\n",
    "2. Distribute weight to homework and final proportional to their correlation coefficients. For example, if the value are the same for both pairs, then both the term homework and midterm should have parameter value 0.5.\n",
    "3. Calculate the value of $\\theta_0$ so that Alice's data fits the model exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1771490677357476\n",
      "0.6700743886411277\n"
     ]
    }
   ],
   "source": [
    "# Calculate the correlation coefficient between homework and final using np.corrcoef()\n",
    "print(np.corrcoef(data['Homework'], data['Final'])[0, 1])\n",
    "\n",
    "# Calculate the correlation coefficient between midterm and final\n",
    "print(np.corrcoef(data['Midterm'], data['Final'])[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribute weight to homework and midterm according to their correlation coefficients.\n",
    "theta1 = 0.2\n",
    "theta2 = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have chosen values for $\\theta_1$ and $\\theta_2$, now we need a value for $\\theta_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "# Solve for theta0 so that if homework=95 and midterm=90, final=93.\n",
    "theta0 = 93 - 0.2 * 95 - 0.8 * 90\n",
    "print(theta0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our guess of the model: $final = 2 + 0.2\\times homework + 0.8\\times midterm$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate the average error the model makes on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[95 90]\n",
      "[93]\n",
      "[2.  0.2 0.8]\n",
      "Squared error for Alice: [0.]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the MSE (mean-squared-error)\n",
    "\n",
    "# 1. calculate the squared error for the first instance\n",
    "# x1 = np.array([95, 90])\n",
    "x1 = data.loc[\"Alice\", ['Homework', \"Midterm\"]].values\n",
    "print(x1)\n",
    "# y1 = np.array([93])\n",
    "y1 = data.loc[\"Alice\", ['Final']].values\n",
    "print(y1)\n",
    "theta = np.array([2.0, 0.2, 0.8])\n",
    "print(theta)\n",
    "prediction = 2.0 + 0.2 * x1[0] + 0.8 * x1[1]\n",
    "squared_error = (prediction - y1) ** 2\n",
    "print(\"Squared error for Alice:\", squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. define a function that returns the error of a given instance.\n",
    "def get_squared_error(data, name, theta):\n",
    "    # Extract x and y from data\n",
    "    x = data.loc[name, ['Homework', 'Midterm']].values\n",
    "    y = data.loc[name, ['Final']].values\n",
    "    # calculate prediction\n",
    "#     prediction = 2.0 + 0.2*x[0]+0.8*x[1]\n",
    "    prediction = theta[0] + theta[1]*x[0] + theta[2]*x[1]\n",
    "    # calculate the squared error\n",
    "    squared_error = (prediction - y)**2\n",
    "    return squared_error\n",
    "\n",
    "get_squared_error(data, \"Bob\", theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.]), array([4.]), array([9.]), array([676.]), array([36.])]\n",
      "MSE: 145.0\n",
      "Root mean squared error (RMSE): 12.041594578792296\n"
     ]
    }
   ],
   "source": [
    "# 3. calculate all errors\n",
    "all_errors = [get_squared_error(data,name,theta) for name in data.index]\n",
    "print(all_errors)\n",
    "\n",
    "# 4. calculate the average.\n",
    "mse = np.mean(all_errors)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"Root mean squared error (RMSE):\", np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilinear Regression: Normal Equation\n",
    "The value of $\\theta$ that minimizes the cost function is:\n",
    "\n",
    "$\\hat{\\theta} = \\big(\\textbf{X}^T\\cdot\\textbf{X}\\big)^{-1}\\cdot\\textbf{X}^T\\cdot\\textbf{y}$.\n",
    "\n",
    "1. $\\textbf{X}$ is an $m\\times (n+1)$ matrix whose i-th row is $\\textbf{x}^{(i)}$.\n",
    "$$\\textbf{X} = \\begin{pmatrix}\n",
    "1 & x^{(1)}_1 & x^{(1)}_2 & \\cdots & x^{(1)}_n \\\\\n",
    "1 & x^{(2)}_1 & x^{(2)}_2 & \\cdots & x^{(2)}_n \\\\\n",
    "\\vdots & \\vdots &\\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x^{(m)}_1 & x^{(m)}_2 & \\cdots & x^{(m)}_n \\\\\n",
    "\\end{pmatrix}$$\n",
    "2. $$\\textbf{y} = \\begin{pmatrix}y^{(1)} \\\\ \\vdots \\\\ y^{(m)}\\end{pmatrix}$$.\n",
    "3. The cost function $J(\\theta)$ also has a matrix expression\n",
    "$$J(\\theta) = \\frac{1}{m}(\\textbf{X}^T\\cdot\\theta - \\textbf{y})^T\\cdot (\\textbf{X}^T\\cdot\\theta - \\textbf{y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.  95.  90.]\n",
      " [  1.  70.  60.]\n",
      " [  1.  80.  80.]\n",
      " [  1. 100.  80.]\n",
      " [  1.  70.  85.]]\n"
     ]
    }
   ],
   "source": [
    "# Construct matrix X using np.hstack(), np.ones()\n",
    "m, n = data.shape\n",
    "X = np.hstack([np.ones([m, 1]), data[['Homework', 'Midterm']].values])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[93]\n",
      " [66]\n",
      " [85]\n",
      " [60]\n",
      " [90]]\n"
     ]
    }
   ],
   "source": [
    "# Construct vector y\n",
    "y = data[['Final']].values\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35.        ]\n",
      " [-0.71627907]\n",
      " [ 1.30697674]]\n"
     ]
    }
   ],
   "source": [
    "# Apply the normal equation to find theta\n",
    "num = X.T.dot(X)\n",
    "theta_opt = np.linalg.inv(num).dot(X.T.dot(y))\n",
    "print(theta_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 36.82790697674422\n",
      "RMSE: 6.068600083770903\n"
     ]
    }
   ],
   "source": [
    "# Find the MSE of the optimal model\n",
    "all_errors = [get_squared_error(data, name, theta_opt) for name in data.index]\n",
    "mse = np.mean(all_errors) # mean squared error\n",
    "rmse = np.sqrt(mse) # root mean squared error\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilinear Regression: Gradient Descent\n",
    "The normal equation is not applicable when $\\textbf{X}^T\\cdot\\textbf{X}$ is not invertible. It happens if:\n",
    "- Several features are linearly dependent (for example, feature3 = feature1 + feature2)\n",
    "- The number of features is greater than the number of training data (for example, DNA data)\n",
    "\n",
    "In this case, we can use the **gradient descent** method to minimize the cost function.\n",
    "\n",
    "Gradient descent is an iterative algorithm for finding the **local minimum** of a differentiable function.\n",
    "- Choose an initial value of $\\hat{\\theta}$ and a **learning rate** $r$.\n",
    "- For each iteration $k$, do:\n",
    "$$\\hat{\\theta} \\leftarrow \\hat{\\theta} - r\\cdot\\frac{\\partial J(\\hat{\\theta})}{\\partial \\theta}.$$\n",
    "- The partial derivative of the cost function is given by\n",
    "$$\n",
    "\\frac{\\partial J(\\hat{\\theta})}{\\partial \\theta} = \\frac{2}{m}\\cdot\\textbf{X}^T\\cdot(\\textbf{X}\\cdot\\theta - \\textbf{y}).\n",
    "$$\n",
    "- **Verify the formula of partial derivative asuuming there is one input feature.**\n",
    "\n",
    "- End iteration if certain stop criteria is reached, such as:\n",
    "    - Value of $\\hat{\\theta}$ becomes stable.\n",
    "    - Certain iteration amount is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 36.29883494]\n",
      " [-69.9576552 ]\n",
      " [127.30733729]]\n"
     ]
    }
   ],
   "source": [
    "# Choose a random initial value\n",
    "m, n = data.shape\n",
    "theta_hat = np.random.rand(n, 1)\n",
    "# print(theta_hat)\n",
    "\n",
    "# # normalize the input values so that numbers are between 0 and 1.\n",
    "X2 = np.hstack([np.ones([m, 1]), data[['Homework', 'Midterm']].values/100])\n",
    "y2 = y\n",
    "# print(X2)\n",
    "\n",
    "num_iter = 6000\n",
    "r = 0.05\n",
    "\n",
    "MSEs = []\n",
    "for iter in range(num_iter):\n",
    "    # Calculate the gradient\n",
    "    gradient = (X2.T).dot(X2.dot(theta_hat)-y2)*2/m\n",
    "                          \n",
    "    # Update the parameters\n",
    "    theta_hat -=  r * gradient\n",
    "    \n",
    "    # Calculate the MSE\n",
    "    MSE = 1/m * (X2.dot(theta_hat) - y2).T.dot(X2.dot(theta_hat) - y2)\n",
    "    \n",
    "    # Append the MSE to the list MSEs\n",
    "    MSEs.append(MSE[0, 0])\n",
    "    \n",
    "print(theta_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[36.91966426]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 200)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(num_iter), MSEs)\n",
    "plt.ylim(0, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "1. Change $r$ to 0.000001 and 1. Observe the MSE curve.\n",
    "2. Does the initial point matter?\n",
    "3. How to determine when to stop the iteration?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilinear Regression via `sklearn.linear_model.LinearRegression`\n",
    "- fit(): train the model\n",
    "- predict(): make predictions\n",
    "- coef_: variable coefficients\n",
    "- intercept_: model intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a linear regression model using LinearRegression from sklearn.linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model_lr = LinearRegression()\n",
    "model_lr.fit(data[['Homework', 'Midterm']], data['Final'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35.00000000000004, array([-0.71627907,  1.30697674]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the parameter values\n",
    "model_lr.intercept_, model_lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "Apply multilinear regression model on the advertising data used in the previous notebook. Train the model in two ways:\n",
    "\n",
    "1. Use `LinearRegression` class from `sklearn.linear_model`.\n",
    "2. Use the normal equation.\n",
    "\n",
    "Split the data into 80% training data and 20% test data. Show the paramter values and the MSEs on the training set and the test set (Hint: use either the formula given in class today or use `mean_squared_error` function from `sklearn.metrics`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
